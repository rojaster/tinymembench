tinymembench v0.4.10 (simple benchmark for memory throughput and latency)

==========================================================================
== Memory bandwidth tests                                               ==
==                                                                      ==
== Note 1: 1MB = 1000000 bytes                                          ==
== Note 2: Results for 'copy' tests show how many bytes can be          ==
==         copied per second (adding together read and writen           ==
==         bytes would have provided twice higher numbers)              ==
== Note 3: 2-pass copy means that we are using a small temporary buffer ==
==         to first fetch data into it, and only then write it to the   ==
==         destination (source -> L1 cache, L1 cache -> destination)    ==
== Note 4: If sample standard deviation exceeds 0.1%, it is shown in    ==
==         brackets                                                     ==
==========================================================================

 C copy backwards                                     :  19772.7 MB/s (1.0%)
 C copy backwards (32 byte blocks)                    :  19758.2 MB/s (0.2%)
 C copy backwards (64 byte blocks)                    :  19758.2 MB/s (0.2%)
 C copy                                               :  19760.6 MB/s (1.9%)
 C copy prefetched (32 bytes step)                    :  19454.3 MB/s
 C copy prefetched (64 bytes step)                    :  19430.7 MB/s
 C 2-pass copy                                        :  19148.6 MB/s (0.2%)
 C 2-pass copy prefetched (32 bytes step)             :  19222.2 MB/s (0.2%)
 C 2-pass copy prefetched (64 bytes step)             :  19197.1 MB/s (0.3%)
 C fill                                               :  28893.3 MB/s (0.7%)
 C fill (shuffle within 16 byte blocks)               :  28574.6 MB/s (0.5%)
 C fill (shuffle within 32 byte blocks)               :  28549.7 MB/s (0.5%)
 C fill (shuffle within 64 byte blocks)               :  28556.2 MB/s (0.4%)
 NEON 64x2 COPY                                       :  30173.2 MB/s (0.3%)
 NEON 64x2x4 COPY                                     :  30186.5 MB/s (0.2%)
 NEON 64x1x4_x2 COPY                                  :  30127.5 MB/s (0.2%)
 NEON 64x2 COPY prefetch x2                           :  28695.7 MB/s (2.6%)
 NEON 64x2x4 COPY prefetch x1                         :  29682.2 MB/s (0.6%)
 NEON 64x2 COPY prefetch x1                           :  29866.5 MB/s (0.5%)
 NEON 64x2x4 COPY prefetch x1                         :  31514.4 MB/s (4.2%)
 ---
 standard memcpy                                      :  30251.8 MB/s (22.9%)
 standard memset                                      :  42933.1 MB/s (8.2%)
 ---
 NEON LDP/STP copy                                    :  30070.9 MB/s (2.0%)
 NEON LDP/STP copy pldl2strm (32 bytes step)          :  29591.5 MB/s (0.8%)
 NEON LDP/STP copy pldl2strm (64 bytes step)          :  29487.0 MB/s
 NEON LDP/STP copy pldl1keep (32 bytes step)          :  29946.7 MB/s (0.6%)
 NEON LDP/STP copy pldl1keep (64 bytes step)          :  29924.1 MB/s (0.2%)
 NEON LD1/ST1 copy                                    :  29918.6 MB/s (0.2%)
 NEON STP fill                                        :  44000.4 MB/s (6.2%)
 NEON STNP fill                                       :  44320.8 MB/s (0.2%)
 ARM LDP/STP copy                                     :  29964.1 MB/s
 ARM STP fill                                         :  47393.3 MB/s (7.1%)
 ARM STNP fill                                        :  48266.6 MB/s (12.2%)

==========================================================================
== Memory latency test                                                  ==
==                                                                      ==
== Average time is measured for random memory accesses in the buffers   ==
== of different sizes. The larger is the buffer, the more significant   ==
== are relative contributions of TLB, L1/L2 cache misses and SDRAM      ==
== accesses. For extremely large buffer sizes we are expecting to see   ==
== page table walk with several requests to SDRAM for almost every      ==
== memory access (though 64MiB is not nearly large enough to experience ==
== this effect to its fullest).                                         ==
==                                                                      ==
== Note 1: All the numbers are representing extra time, which needs to  ==
==         be added to L1 cache latency. The cycle timings for L1 cache ==
==         latency can be usually found in the processor documentation. ==
== Note 2: Dual random read means that we are simultaneously performing ==
==         two independent memory accesses at a time. In the case if    ==
==         the memory subsystem can't handle multiple outstanding       ==
==         requests, dual random read has the same timings as two       ==
==         single reads performed one after another.                    ==
==========================================================================

block size : single random read / dual random read
      1024 :    0.0 ns          /     0.0 ns 
      2048 :    0.0 ns          /     0.0 ns 
      4096 :    0.0 ns          /     0.0 ns 
      8192 :    0.0 ns          /     0.0 ns 
     16384 :    0.0 ns          /     0.0 ns 
     32768 :    0.0 ns          /     0.0 ns 
     65536 :    0.0 ns          /     0.0 ns 
    131072 :    0.0 ns          /     0.0 ns 
    262144 :    2.2 ns          /     3.3 ns 
    524288 :    3.3 ns          /     4.2 ns 
   1048576 :    3.9 ns          /     4.5 ns 
   2097152 :    4.1 ns          /     4.6 ns 
   4194304 :    5.0 ns          /     5.6 ns 
   8388608 :    6.4 ns          /     6.8 ns 
  16777216 :   15.0 ns          /    21.2 ns 
  33554432 :   48.8 ns          /    75.0 ns 
  67108864 :   74.4 ns          /    96.4 ns 
